# Оценка качества RAG: метрики и расчёт

В бенчмарке тестовые запросы генерируются из чанков (LLM), ручных эталонов нет. Ниже — как считаются метрики retrieval и end-to-end и как интерпретировать результаты. Файл отчёта (например `report.txt`) содержит те же метрики в виде JSON и таблиц.

---

## 1. Метрики retrieval

Используются: **ranked_ids** — порядок выдачи системы (id чанков), **gold_ids** — множество релевантных id (эталон). Считаются при **k = 1, 3, 5, 10, 20**.

### Recall@k

- **Что измеряет:** доля запросов, у которых хотя бы один релевантный документ попал в топ-k.
- **Расчёт (на одном запросе):** берём первые k элементов из **ranked_ids**; если среди них есть хотя бы один id из **gold_ids** → 1, иначе → 0. По всем запросам: **Recall@k = (сумма по запросам) / N**.
- **Зачем:** без релевантного чанка в выдаче LLM не даст правильный ответ.

### MRR@k (Mean Reciprocal Rank)

- **Что измеряет:** на какой позиции в топ-k впервые встречается релевантный документ (в среднем).
- **Расчёт (на одном запросе):** находим первую позицию `pos` (0-based) в **ranked_ids**, где id в **gold_ids**; значение = **1 / (pos + 1)**. Если в топ-k нет релевантного → 0. По всем запросам: **MRR@k = (сумма 1/rank) / N**.
- **Зачем:** учитывает не только «попал ли», но и «как высоко» — важно для ограниченного контекста LLM.

### nDCG@k

- **Что измеряет:** насколько хорошо релевантные документы расставлены вверху списка (с дисконтом по позиции).
- **Расчёт:** **DCG@k** — для каждой позиции i в топ-k: если id релевантный, добавляем **1 / log₂(i + 2)**. **IDCG@k** — DCG при идеальном порядке. **nDCG@k = DCG@k / IDCG@k** (значение в [0, 1]).
- **Зачем:** штрафует случай, когда релевантный документ есть в топ-k, но далеко внизу.

---

## 2. Метрики end-to-end

Сравниваются **ответ модели (pred)** и **эталон (ref)**. Перед сравнением применяется нормализация: lowercase, удаление лишних символов, схлопывание пробелов.

### Semantic Similarity (основная e2e-метрика)

- **Что измеряет:** смысловую близость ответа эталону (не буквальное совпадение).
- **Расчёт:** тексты нормализуются, кодируются моделью эмбеддингов (та же, что в RAG, префикс `passage:`), векторы нормируются; **Semantic Similarity = скалярное произведение** (косинусное сходство), значение в [0, 1].
- **Интерпретация:** **0.8+** — уровень production-grade RAG; **0.85+** — высокое качество.

### F1 по токенам (F1_token)

- **Что измеряет:** перекрытие по словам между ответом и эталоном.
- **Расчёт:** после нормализации тексты разбиваются на токены (слова); **Precision** = |пересечение| / |ответ|, **Recall** = |пересечение| / |эталон|; **F1_token = 2·P·R / (P+R)**.
- **Зачем:** дополняет Semantic Similarity лексическим перекрытием (термины, команды); при перефразировании F1 может быть умеренным при высокой Semantic Similarity.

EM и ROUGE-L в коде реализованы, но в отчёте e2e по умолчанию выводятся только **Semantic Similarity** и **F1_token**.

---

## 3. Методология и интерпретация

- **Literal QA** — вопрос «в лоб» по фрагменту; Recall@20 обычно 0.90–0.98.
- **Semantic (Paraphrase + Scenario)** — перефразирования и сценарные вопросы; Recall@20 ожидаемо ниже (порядка 0.55–0.85). Снижение при переходе к semantic — ожидаемая плата за реалистичность бенчмарка.
- Retrieval в отчёте показывается **раздельно по типам** (`--eval-mode all`): Literal vs Paraphrase/Scenario.
- Результаты в файле отчёта (например `report.txt`) соответствуют этим метрикам; пример интерпретации приведён в основном [README](../README.md#оценка-качества-и-результаты).

---

## 4. Запуск

```bash
python main.py eval
# или с параметрами:
python -m rag_benchmark.run_all --samples 30 --e2e-limit 20 --eval-mode all --output report.json
```

- **--samples** — число чанков для генерации QA.
- **--e2e-limit** — лимит примеров для e2e.
- **--eval-mode** — `all` (разбивка по типам), `literal` или `semantic`.
- **--skip-e2e** — только retrieval.
- **--output** — путь к файлу отчёта.

---

## 5. Структура модуля

```
rag_benchmark/
├── generate_queries.py   # чанки → LLM → question, answer, relevant_chunk_ids
├── evaluate_retrieval.py # Recall@k, MRR@k, nDCG@k
├── evaluate_end2end.py   # Semantic Similarity, F1_token
├── metrics.py            # формулы метрик
└── run_all.py            # генерация + retrieval + e2e
```

Данный документ: **docs/Оценка_качества_RAG.md**.
